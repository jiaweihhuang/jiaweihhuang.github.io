---
layout: page
permalink: /Publication/
title: Publications
description: 
years: []
nav: false
nav_order: 1
# permalink: /publications/
# title: publications
# description: publications by categories in reversed chronological order. generated by jekyll-scholar.
# years: [1967, 1956, 1950, 1935, 1905]
---

<div class="Publications">

{%- for y in page.years %}
  <h2 class="year">{{y}}</h2>
  {% bibliography -f papers -q @*[display_year={{y}}]* %}
{% endfor %}


</div>

<section id="" style="width:1200px">
    <header class="major">
        <ul>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                 Robust Knowledge Transfer in Tiered Reinforcement Learning
                </span>
                <a href="https://arxiv.org/abs/2302.05534" style="color:cornflowerblue">
                    (arxiv) </a> <br> 
                    [Preprint] <strong>Jiawei Huang</strong>, Niao He
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                Tiered Reinforcement Learning: Pessimism in the Face of Uncertainty and Constant Regret
                </span>
                <a href="https://arxiv.org/abs/2205.12418" style="color:cornflowerblue">
                    (arxiv) </a> <a href="https://github.com/jiaweihhuang/Tiered-RL-Experiments" style="color:cornflowerblue">[Code]</a><br> 
                    [NeurIPS 2022] <strong>Jiawei Huang</strong>, Li Zhao, Tao Qin, Wei Chen, Nan Jiang, Tie-Yan Liu
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                A Minimax Learning Approach to Off-Policy Evaluation in Confounded POMDP
                </span>
                <a href="https://arxiv.org/abs/2111.06784" style="color:cornflowerblue">
                    (arxiv) </a> <a href="https://github.com/jiaweihhuang/Confounded-POMDP-Exp" style="color:cornflowerblue">[Code]</a><br> 
                    [ICML 2022; <strong><font color="red">Long Talk</font></strong>] Chengchun Shi, Masatoshi Uehara, <strong>Jiawei Huang</strong>, Nan Jiang
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality
                </span>
                <a href="https://openreview.net/forum?id=ccWaPGl9Hq" style="color:cornflowerblue">
                    (OpenReview) </a> <br> 
                    [ICLR 2022; <strong><font color="red">Spotlight</font></strong>] <strong>Jiawei Huang</strong>, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, Tie-Yan Liu
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio Correction
                </span>
                <a href="https://arxiv.org/abs/2106.00993" style="color:cornflowerblue">
                    (arxiv) </a> <br> 
                    [AISTATS 2022] <strong>Jiawei Huang</strong>, Nan Jiang
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                Minimax Value Interval for Off-Policy Evaluation and Policy Optimization
                </span>
                <a href="https://arxiv.org/abs/2002.02081" style="color:cornflowerblue">
                    (arxiv) </a><a href="https://github.com/jiaweihhuang/Minimax-Value-Interval"  style="color:cornflowerblue">[Code]</a> <br> 
                    [NeurIPS 2020] Nan Jiang, <strong>Jiawei Huang</strong>
            </font>
            </h3>
        </li>
        <li> 
            <h3>
                <font size="4">
                <span style="font-weight:1000;">
                From Importance Sampling to Doubly Robust Policy Gradient
                </span>
                <a href="https://arxiv.org/abs/1910.09066" style="color:cornflowerblue">
                    (arxiv) </a><a href="https://github.com/jiaweihhuang/DR-PG"  style="color:cornflowerblue">[Code]</a> <br> 
                [ICML 2020] <strong>Jiawei Huang</strong>, Nan Jiang
                </font>
            </h3>
        </li>
        <li> 
            <h3>
            <font size="4">
            <span style="font-weight:1000;">
                Minimax Weight and Q-Function Learning for Off-Policy Evaluation
                </span>
                <a href="https://arxiv.org/abs/1910.09066" style="color:cornflowerblue">
                    (arxiv) </a><br> 
                [ICML 2020] Masatoshi Uehara, <strong>Jiawei Huang</strong>, Nan Jiang
            </font>
            </h3>
        </li>
        <li> 
            <h3>
            <font size="4">
            <span style="font-weight:1000;">
                WeightNet: Revisiting the Design Space of Weight Networks
                </span>
                <a href="https://arxiv.org/abs/2007.11823" style="color:cornflowerblue">
                    (arxiv) </a><br> 
                [ECCV 2020] Ningning Ma, Xiangyu Zhang, <strong>Jiawei Huang</strong>, Jian Sun
            </font>
            </h3>
        </li>
        </ul>
    </header>
</section>